{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71096a01-1d44-403d-95d5-dd9bf4fd52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from Bio import SeqIO\n",
    "from scipy.special import softmax\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71637cd9-917b-4c78-a221-df7eed150263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "reference_protein = 'TTSAGESADPVTATVENYGGETQVQRRQHTDIAFILDRFVKVKPKEQVNVLDLMQIPAHTLVGALLRTATYYFSDLELAVKHEGDLTWVPNGAPETALDNTTNPTAYHKEPLTRLALPYTAPHRVLATVYNGSSKYGDTSTNNVRGDLQVLAQKAERTLPTSFNFGAIKATRVTELLYRMKRAETYCPRPLLAIQPSDARHKQRIVAPAKQ'\n",
    "print(len(reference_protein))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc3eab-7591-4b46-a058-dec957badbc5",
   "metadata": {},
   "source": [
    "## Terminologies\n",
    "DMS: deep mutational scanning is used to study the impact of mutations on protein structure and function\n",
    "Grammaticality: the distance between original and mutated embeddings - see Hie 2020 \"Learning mutational semantics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d5d580e-cfa0-475b-b8e3-bc3df229dd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F: using the original seq and the desired mutation(s), return the mutated sequence\n",
    "def mutate_sequence(reference_sequence,mutations):\n",
    "    mutated_seq = reference_sequence\n",
    "    for mutation in mutations:\n",
    "        if 'ins' not in mutation and 'del' not in mutation and \"X\" not in mutation:\n",
    "            mutant_amino = mutation[-1]\n",
    "            mutant_pos = int(mutation[1:-1])\n",
    "            mutated_seq = mutated_seq[:mutant_pos-1]+mutant_amino+mutated_seq[mutant_pos:]\n",
    "    return mutated_seq\n",
    "\n",
    "# F: generates a list of sequences where every position in the protein sequence is mutated to every possible amino acid by default\n",
    "# Note: this is only for single-residue mutation - need to update to deal with multiple mutations per sequence\n",
    "def DMS(reference,start=0,end = None):\n",
    "  if end == None:\n",
    "    end = len(reference)\n",
    "  seq_list = []\n",
    "  amino_acids = [\"A\",\"R\",\"N\",\"D\",\"C\",\"Q\",\"E\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"Y\",\"V\"]\n",
    "  for i,ref_amino_acid in enumerate(reference):\n",
    "      if i>=start and i<=end:\n",
    "        for mutant_amino_acid in amino_acids:\n",
    "            mutated_seq = reference[:i]+mutant_amino_acid+reference[i+1:]\n",
    "            seq = SeqRecord(Seq(mutated_seq), id=ref_amino_acid+str(i+1)+mutant_amino_acid)\n",
    "            seq_list.append(seq)\n",
    "  return seq_list\n",
    "\n",
    "seqs_of_mutations = DMS(reference_protein,138,143)\n",
    "len(seqs_of_mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9377b2ab-3682-4d64-9291-a455e7c51dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: use PLM to extract embedding and logits for a given mutation\n",
    "def embed_sequence(sequence,model,device,model_layers,batch_converter):\n",
    "    #Sequences to embed (We only embed the reference and use the probabilities from that to generate the scores)\n",
    "    sequence_data = [('base', sequence)]\n",
    "\n",
    "    #Get tokens etc\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(sequence_data)\n",
    "    batch_len = (batch_tokens != alphabet.padding_idx).sum(1)[0]\n",
    "\n",
    "    #Move tokens to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        batch_tokens = batch_tokens.to(device=device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[model_layers], return_contacts=False)\n",
    "    del batch_tokens\n",
    "\n",
    "    #Embed Sequences\n",
    "    token_representation = results[\"representations\"][model_layers][0]\n",
    "    full_embedding = token_representation[1:batch_len - 1].cpu()\n",
    "    base_mean_embedding  = token_representation[1 : batch_len - 1].mean(0).cpu()\n",
    "\n",
    "    #Get Embedding and probabilities for reference sequence (Should be first sequence in data)\n",
    "    lsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "    base_logits = lsoftmax((results[\"logits\"][0]).to(device=\"cpu\"))\n",
    "    return results, base_logits, base_mean_embedding,full_embedding\n",
    "\n",
    "# F: process embeddings and logits for sequence and return as a dictionary\n",
    "def process_protein_sequence(sequence,model,model_layers,batch_converter):\n",
    "    #Embed Sequence\n",
    "    base_seq = sequence\n",
    "    results,base_logits, base_mean_embedding, full_embedding = embed_sequence(base_seq,model,device,model_layers,batch_converter)\n",
    "    results_dict = {}\n",
    "    results_dict[\"Mean_Embedding\"] = base_mean_embedding.tolist()\n",
    "    # results_dict[\"Full_Embedding\"] = full_embedding.tolist()\n",
    "    results_dict[\"Logits\"] = base_logits.tolist()\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b76a99-e02b-4d1b-85b1-ef46dab1ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: \n",
    "def grammaticality_and_evolutionary_index(word_pos_prob, seq, mutations):\n",
    "    if len(mutations) == 0:\n",
    "        print('No mutations detected')\n",
    "        return 0, 0\n",
    "    mut_probs = []\n",
    "    ev_ratios = []\n",
    "    current_support = -1\n",
    "    print('Mutations: ', mutations)\n",
    "    for mutation in mutations:\n",
    "        #Ignore insertions\n",
    "        if 'ins' not in mutation and 'del' not in mutation and \"X\" not in mutation:\n",
    "            #Split mutation \n",
    "            aa_orig = mutation[0]\n",
    "            aa_pos = int(mutation[1:-1]) - 1\n",
    "            aa_mut = mutation[-1]\n",
    "            if (seq[aa_pos] != aa_orig):\n",
    "                print(mutation)\n",
    "            assert(seq[aa_pos] == aa_orig)\n",
    "\n",
    "            #Get probabilities for changes\n",
    "            prob_change = word_pos_prob[(aa_mut, aa_pos)]\n",
    "            prob_original = word_pos_prob[(aa_orig, aa_pos)]\n",
    "            #Log probabilities to allow for subtraction\n",
    "            ev_ratio = prob_change - prob_original\n",
    "            ev_ratios.append(ev_ratio)\n",
    "\n",
    "            #Log probabilities to allow for sum rather than product\n",
    "            mut_probs.append(word_pos_prob[(aa_mut, aa_pos)])\n",
    "    return np.sum(mut_probs), np.sum(ev_ratios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
