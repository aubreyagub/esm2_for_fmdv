{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a7802e",
   "metadata": {
    "collapsed": false,
    "id": "8pdVt-QrreVd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from Bio import SeqIO\n",
    "from scipy.special import softmax\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd64e93",
   "metadata": {
    "id": "EIDK9H-dsj4-",
    "tags": []
   },
   "source": [
    "# Compression Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0308b416",
   "metadata": {
    "collapsed": false,
    "id": "FNQLBzZqrsHV",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "def compressed_pickle(title, data):\n",
    "  with bz2.BZ2File(title + '.pbz2', 'w') as f:\n",
    "    cPickle.dump(data, f)\n",
    "\n",
    "def decompress_pickle(file):\n",
    "  data = bz2.BZ2File(file, 'rb')\n",
    "  data = cPickle.load(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520e38f",
   "metadata": {
    "id": "dvifJu39sg89",
    "tags": []
   },
   "source": [
    "# Genbank Annotation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e97909",
   "metadata": {
    "collapsed": false,
    "id": "AD5YaKaqri0i",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def makeOrfTable(genbank_record):\n",
    "    orfs=[]\n",
    "    for feature in genbank_record.features:\n",
    "        if feature.type ==\"CDS\":\n",
    "            orf = feature.qualifiers['gene'][0]\n",
    "            for i, locations in enumerate(feature.location.parts):\n",
    "                orfs.append([orf, locations.start, locations.end, i, locations])\n",
    "    orfs = pd.DataFrame(orfs)\n",
    "    orfs.columns = ['ORF','Start','End','Part','Locations']\n",
    "    orfs = orfs.set_index(\"ORF\")\n",
    "    return orfs\n",
    "\n",
    "def makeMatProteinTable(genbank_record):\n",
    "    proteins=[]\n",
    "    for feature in genbank_record.features:\n",
    "        if feature.type ==\"mat_peptide\":\n",
    "            protein = feature.qualifiers['product'][0]\n",
    "            orf = feature.qualifiers['gene'][0]\n",
    "            for i, locations in enumerate(feature.location.parts):\n",
    "                proteins.append([protein, orf ,locations.start, locations.end, i, locations])\n",
    "    proteins = pd.DataFrame(proteins)\n",
    "    proteins.columns = ['Protein',\"ORF\",'Start','End','Part','Locations']\n",
    "    proteins = proteins.set_index(\"Protein\")\n",
    "    return proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121b63e",
   "metadata": {
    "id": "_6-DG2B3seTT",
    "tags": []
   },
   "source": [
    "# Mutation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705aead7",
   "metadata": {
    "collapsed": false,
    "id": "PIQVJIjErmfH",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def mutate_sequence(reference_sequence,mutations):\n",
    "    mutated_seq = reference_sequence\n",
    "    for mutation in mutations:\n",
    "        if 'ins' not in mutation and 'del' not in mutation and \"X\" not in mutation:\n",
    "            mutant_amino = mutation[-1]\n",
    "            mutant_pos = int(mutation[1:-1])\n",
    "            mutated_seq = mutated_seq[:mutant_pos-1]+mutant_amino+mutated_seq[mutant_pos:]\n",
    "    return mutated_seq\n",
    "\n",
    "# This function generates a list of sequences where every position in the protein sequence is mutated to every possible amino acid by default\n",
    "# For my application, I only want a list of mutated sequences for the 6 positions I want to mutate \n",
    "def DMS(reference,start=0,end = None):\n",
    "  if end == None:\n",
    "    end = len(reference)\n",
    "  seq_list = []\n",
    "  amino_acids = [\"A\",\"R\",\"N\",\"D\",\"C\",\"Q\",\"E\",\"G\",\"H\",\"I\",\"L\",\"K\",\"M\",\"F\",\"P\",\"S\",\"T\",\"W\",\"Y\",\"V\"]\n",
    "  for i,ref_amino_acid in enumerate(reference):\n",
    "\n",
    "      if i>=start and i<=end:\n",
    "        for mutant_amino_acid in amino_acids:\n",
    "            mutated_seq = reference[:i]+mutant_amino_acid+reference[i+1:]\n",
    "            seq = SeqRecord(Seq(mutated_seq), id=ref_amino_acid+str(i+1)+mutant_amino_acid)\n",
    "            seq_list.append(seq)DMS\n",
    "  return seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790056f",
   "metadata": {
    "id": "cb_ttyiKsKOb",
    "tags": []
   },
   "source": [
    "# Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c191577f",
   "metadata": {
    "collapsed": false,
    "id": "nU--vguRryE4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def iterative_translate(sequence,truncate_proteins=False):\n",
    "    amino_acid = \"\"\n",
    "    for i in range(0,len(sequence)-2,3):\n",
    "        codon = str(sequence[i:i+3])\n",
    "        codon = codon.replace(\"?\", \"N\")\n",
    "        if \"-\" in codon:\n",
    "            if codon == \"---\":\n",
    "                amino_acid +=\"-\"\n",
    "            else:\n",
    "                amino_acid+= \"X\"\n",
    "        else:\n",
    "            amino_acid += str(Seq(codon).translate())\n",
    "    if truncate_proteins == True:\n",
    "        if \"*\" in amino_acid:\n",
    "            amino_acid = amino_acid[:amino_acid.index(\"*\")]\n",
    "    return amino_acid\n",
    "\n",
    "def translate_with_genbank(sequence,ref):\n",
    "    orfs = makeOrfTable(ref)\n",
    "    translated_sequence = {orfs.index[i]+\":\"+str(orfs.iloc[i].Part):{\"Sequence\":\"\".join(list(iterative_translate(\"\".join(orfs.iloc[i].Locations.extract(sequence)),truncate_proteins=True))),\"ORF\":orfs.index[i]} for i in range(len(orfs))}\n",
    "    return translated_sequence\n",
    "\n",
    "def translate_mat_proteins_with_genbank(sequence,ref):\n",
    "    proteins = makeMatProteinTable(ref)\n",
    "    proteins = proteins.drop_duplicates(subset=[\"ORF\",'Start','End','Part',],keep=\"first\")\n",
    "    proteins_dict={}\n",
    "    for i in range(len(proteins)):\n",
    "        protein = \"\".join(list(iterative_translate(\"\".join(proteins.iloc[i].Locations.extract(sequence)),truncate_proteins=True)))\n",
    "        if proteins.index[i] in proteins_dict:\n",
    "            proteins_dict[proteins.index[i]][\"Sequence\"] = proteins_dict[proteins.index[i]][\"Sequence\"]+protein\n",
    "        else:\n",
    "            proteins_dict[proteins.index[i]] = {\"Sequence\":protein, \"ORF\":proteins.iloc[i].ORF, \"Part\":proteins.iloc[i].Part}\n",
    "    # translated_sequence = {proteins.index[i]:{\"Sequence\":\"\".join(list(iterative_translate(\"\".join(proteins.iloc[i].Locations.extract(sequence)),truncate_proteins=True))), \"ORF\":proteins.iloc[i].ORF} }\n",
    "    return proteins_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ac927",
   "metadata": {
    "id": "pXEM06W3sSGp",
    "tags": []
   },
   "source": [
    "# Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1db988e",
   "metadata": {
    "collapsed": false,
    "id": "AXgDlovAr0tb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def embed_sequence(sequence,model,device,model_layers,batch_converter):\n",
    "    #Sequences to embed (We only embed the reference and use the probabilities from that to generate the scores)\n",
    "    sequence_data = [('base', sequence)]\n",
    "\n",
    "    #Get tokens etc\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(sequence_data)\n",
    "    batch_len = (batch_tokens != alphabet.padding_idx).sum(1)[0]\n",
    "\n",
    "    #Move tokens to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        batch_tokens = batch_tokens.to(device=device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[model_layers], return_contacts=False)\n",
    "    del batch_tokens\n",
    "\n",
    "    #Embed Sequences\n",
    "    token_representation = results[\"representations\"][model_layers][0]\n",
    "    full_embedding = token_representation[1:batch_len - 1].cpu()\n",
    "    base_mean_embedding  = token_representation[1 : batch_len - 1].mean(0).cpu()\n",
    "\n",
    "    #Get Embedding and probabilities for reference sequence (Should be first sequence in data)\n",
    "    lsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "    base_logits = lsoftmax((results[\"logits\"][0]).to(device=\"cpu\"))\n",
    "    return results, base_logits, base_mean_embedding,full_embedding\n",
    "\n",
    "def process_protein_sequence(sequence,model,model_layers,batch_converter):\n",
    "    #Embed Sequence\n",
    "    base_seq = sequence\n",
    "    results,base_logits, base_mean_embedding, full_embedding = embed_sequence(base_seq,model,device,model_layers,batch_converter)\n",
    "    results_dict = {}\n",
    "    results_dict[\"Mean_Embedding\"] = base_mean_embedding.tolist()\n",
    "    # results_dict[\"Full_Embedding\"] = full_embedding.tolist()\n",
    "    results_dict[\"Logits\"] = base_logits.tolist()\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d35fb1",
   "metadata": {
    "id": "km9TFBkosUkd",
    "tags": []
   },
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd922ea",
   "metadata": {
    "collapsed": false,
    "id": "QBUfC3fur6J7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def grammaticality_and_evolutionary_index(word_pos_prob, seq, mutations):\n",
    "    if len(mutations) == 0:\n",
    "        print('No mutations detected')\n",
    "        return 0, 0\n",
    "    mut_probs = []\n",
    "    ev_ratios = []\n",
    "    current_support = -1\n",
    "    print('Mutations: ', mutations)\n",
    "    for mutation in mutations:\n",
    "        #Ignore insertions\n",
    "        if 'ins' not in mutation and 'del' not in mutation and \"X\" not in mutation:\n",
    "            #Split mutation \n",
    "            aa_orig = mutation[0]\n",
    "            aa_pos = int(mutation[1:-1]) - 1\n",
    "            aa_mut = mutation[-1]\n",
    "            if (seq[aa_pos] != aa_orig):\n",
    "                print(mutation)\n",
    "            assert(seq[aa_pos] == aa_orig)\n",
    "\n",
    "            #Get probabilities for changes\n",
    "            prob_change = word_pos_prob[(aa_mut, aa_pos)]\n",
    "            prob_original = word_pos_prob[(aa_orig, aa_pos)]\n",
    "            #Log probabilities to allow for subtraction\n",
    "            ev_ratio = prob_change - prob_original\n",
    "            ev_ratios.append(ev_ratio)\n",
    "\n",
    "            #Log probabilities to allow for sum rather than product\n",
    "            mut_probs.append(word_pos_prob[(aa_mut, aa_pos)])\n",
    "    return np.sum(mut_probs), np.sum(ev_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c8322",
   "metadata": {
    "id": "zT8H4935sW6D",
    "tags": []
   },
   "source": [
    "# Genbank Funnctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bafbb6",
   "metadata": {
    "collapsed": false,
    "id": "45a78773-5762-4cbd-8c30-77e1fc8802e9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def process_sequence_genbank(sequence,genbank,model,model_layers):\n",
    "    #Translate nucleotide to proteins using genbank\n",
    "    Coding_Regions= translate_with_genbank(sequence,genbank)\n",
    "    Mature_Proteins= translate_mat_proteins_with_genbank(sequence,genbank)\n",
    "    polyprotein_orfs =set([Mature_Proteins[prot][\"ORF\"] for prot in Mature_Proteins.keys()])\n",
    "    Filtered_Coding_Regions = {**Coding_Regions}\n",
    "    for orf in Coding_Regions.keys():\n",
    "        if Coding_Regions[orf][\"ORF\"] in polyprotein_orfs:\n",
    "            del Filtered_Coding_Regions[orf]\n",
    "    Merged_Coding_Regions = {**Filtered_Coding_Regions,**Mature_Proteins}\n",
    "    #Embed Sequence\n",
    "    for key,value in Merged_Coding_Regions.items():\n",
    "        base_seq = Merged_Coding_Regions[key][\"Sequence\"]\n",
    "        results,base_logits, base_mean_embedding, full_embedding = embed_sequence(base_seq,model,device,model_layers,batch_converter)\n",
    "        word_pos_prob = {}\n",
    "        for pos in range(len(base_seq)):\n",
    "            for word in alphabet.all_toks:\n",
    "                word_idx = alphabet.get_idx(word)\n",
    "                prob = base_logits[pos + 1, word_idx]\n",
    "                word_pos_prob[(word, pos)] = prob\n",
    "        value[\"Mean_Embedding\"] = base_mean_embedding.tolist()\n",
    "        # value[\"Full_Embedding\"] = full_embedding.tolist()\n",
    "        value[\"Logits\"] = base_logits.tolist()\n",
    "\n",
    "    all_embeddings = [np.array(Merged_Coding_Regions [key][\"Mean_Embedding\"]) for key in Merged_Coding_Regions.keys()]\n",
    "#     Merged_Coding_Regions [\"Sum_Embedding\"] = list(np.sum(all_embeddings,axis=0))\n",
    "#     Merged_Coding_Regions [\"Concatenated_Embedding\"] = list(np.concatenate(all_embeddings))\n",
    "    return Merged_Coding_Regions\n",
    "\n",
    "\n",
    "def get_sequence_grammaticality(sequence,sequence_logits):   \n",
    "    prob_list = []\n",
    "    sequence_logits = torch.FloatTensor(sequence_logits)\n",
    "    for pos in range(len(sequence)):\n",
    "        word_idx = alphabet.get_idx(sequence[pos])\n",
    "        word = sequence_logits[(pos + 1,word_idx)]\n",
    "        prob_list.append(word)\n",
    "    base_grammaticality =np.sum(prob_list)\n",
    "    return base_grammaticality\n",
    "\n",
    "\n",
    "def process_and_dms_sequence_genbank(sequence,genbank,model,model_layers,specify_orf=\"\"):\n",
    "    #Translate nucleotide to proteins using genbank\n",
    "    Coding_Regions= translate_with_genbank(sequence,genbank)\n",
    "    Mature_Proteins= translate_mat_proteins_with_genbank(sequence,genbank)\n",
    "    polyprotein_orfs =set([Mature_Proteins[prot][\"ORF\"] for prot in Mature_Proteins.keys()])\n",
    "    Filtered_Coding_Regions = {**Coding_Regions}\n",
    "    for orf in Coding_Regions.keys():\n",
    "        if Coding_Regions[orf][\"ORF\"] in polyprotein_orfs:\n",
    "            del Filtered_Coding_Regions[orf]\n",
    "    Merged_Coding_Regions = {**Filtered_Coding_Regions,**Mature_Proteins}\n",
    "    embeddings = {}\n",
    "    if specify_orf !=\"\":\n",
    "        Merged_Coding_Regions = {specify_orf:Merged_Coding_Regions[specify_orf]}\n",
    "    #Embed Sequence\n",
    "    for key,value in Merged_Coding_Regions.items():\n",
    "        embeddings[key] = {}\n",
    "        base_seq = Merged_Coding_Regions[key][\"Sequence\"]\n",
    "        results,base_logits, base_mean_embedding, full_embedding = embed_sequence(base_seq,model,device,model_layers,batch_converter)\n",
    "        word_pos_prob = {}\n",
    "        for pos in range(len(base_seq)):\n",
    "            for word in alphabet.all_toks:\n",
    "                word_idx = alphabet.get_idx(word)\n",
    "                prob = base_logits[pos + 1, word_idx]\n",
    "                word_pos_prob[(word, pos)] = prob\n",
    "        embeddings[key][\"Reference\"] = {\"Mean_Embedding\":base_mean_embedding.tolist(),\n",
    "                                        \"Logits\":base_logits.tolist(),\n",
    "                                        \"sequence_grammaticality\":get_sequence_grammaticality(base_seq,base_logits)\n",
    "                                     }\n",
    "        # Now DMS the sequence and embed and measure to reference\n",
    "        sequences = DMS(base_seq)\n",
    "        for fasta in tqdm(sequences):\n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "#             print(key,name)\n",
    "            mutations = [name]\n",
    "            embeddings[key][name] = process_protein_sequence(sequence,model,model_layers,batch_converter)\n",
    "            # L1/Manhattan Distance between mean embeddings used for the semantic change\n",
    "            semantic_change = float(sum(abs(target-base) for target, base in zip(embeddings[key][\"Reference\"][\"Mean_Embedding\"],\n",
    "                                                                                 embeddings[key][name] [\"Mean_Embedding\"])))\n",
    "            gm, ev = grammaticality_and_evolutionary_index(word_pos_prob, base_seq, mutations)\n",
    "#             print('Semantic score: ', semantic_change)\n",
    "#             print('Grammaticality: ', gm)\n",
    "#             print('Relative Grammaticality: ', ev)\n",
    "            embeddings[key][name][\"label\"] = name\n",
    "            embeddings[key][name][\"semantic_score\"] = semantic_change\n",
    "            #Probability of mutation, given the reference sequence\n",
    "            embeddings[key][name][\"grammaticality\"] = gm\n",
    "            embeddings[key][name][\"relative_grammaticality\"] = ev\n",
    "            #Probability of whole sequence\n",
    "            embeddings[key][name]['sequence_grammaticality'] = get_sequence_grammaticality(sequence,embeddings[key][name]['Logits'])\n",
    "#             print('Sequence Grammaticality: ', embeddings[key][name]['sequence_grammaticality'])\n",
    "            #Probability ratio between the mutant sequence and the reference sequence\n",
    "            embeddings[key][name]['relative_sequence_grammaticality'] = embeddings[key][name]['sequence_grammaticality']-embeddings[key][\"Reference\"]['sequence_grammaticality']\n",
    "#             print('Relative Sequence Grammaticality: ', embeddings[key][name]['relative_sequence_grammaticality'])\n",
    "            embeddings[key][name][\"probability\"] = np.exp(gm)\n",
    "#             print(embeddings[key][name]['grammaticality'])\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_mutations(seq1, seq2):\n",
    "    mutations = []\n",
    "    for i in range(len(seq1)):\n",
    "        if seq1[i] != seq2[i]:\n",
    "            if seq1[i] != '-' and seq2[i] == '-':\n",
    "                mutations.append('{}{}del'.format(seq1[i], i + 1))\n",
    "            else:\n",
    "                mutations.append('{}{}{}'.format(seq1[i] , i + 1, seq2[i]))\n",
    "    return mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3931356f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def single_protein_DMS(key,protein_sequence,model,model_layers,batch_converter,alphabet,device,start,end):\n",
    "    embeddings = {}\n",
    "    embeddings[key] = {}\n",
    "    base_seq = protein_sequence\n",
    "    results,base_logits, base_mean_embedding, full_embedding = embed_sequence(base_seq,model,device,model_layers,batch_converter)\n",
    "    word_pos_prob = {}\n",
    "    for pos in range(len(base_seq)):\n",
    "        for word in alphabet.all_toks:\n",
    "            word_idx = alphabet.get_idx(word)\n",
    "            prob = base_logits[pos + 1, word_idx]\n",
    "            word_pos_prob[(word, pos)] = prob\n",
    "    embeddings[key][\"Reference\"] = {\"Mean_Embedding\":base_mean_embedding.tolist(),\n",
    "                                    \"Logits\":base_logits.tolist(),\n",
    "                                    \"sequence_grammaticality\":get_sequence_grammaticality(base_seq,base_logits)\n",
    "                                }\n",
    "    # Now DMS the sequence and embed and measure to reference\n",
    "    sequences = DMS(protein_sequence,start,end)\n",
    "    for fasta in tqdm(sequences):\n",
    "        name, sequence = fasta.id, str(fasta.seq)\n",
    "#             print(key,name)\n",
    "        mutations = [name]\n",
    "        embeddings[key][name] = process_protein_sequence(sequence,model,model_layers,batch_converter)\n",
    "        # L1/Manhattan Distance between mean embeddings used for the semantic change\n",
    "        semantic_change = float(sum(abs(target-base) for target, base in zip(embeddings[key][\"Reference\"][\"Mean_Embedding\"],\n",
    "                                                                            embeddings[key][name] [\"Mean_Embedding\"])))\n",
    "        gm, ev = grammaticality_and_evolutionary_index(word_pos_prob, base_seq, mutations)\n",
    "#             print('Semantic score: ', semantic_change)\n",
    "#             print('Grammaticality: ', gm)\n",
    "#             print('Relative Grammaticality: ', ev)\n",
    "        embeddings[key][name][\"label\"] = name\n",
    "        embeddings[key][name][\"semantic_score\"] = semantic_change\n",
    "        #Probability of mutation, given the reference sequence\n",
    "        embeddings[key][name][\"grammaticality\"] = gm\n",
    "        embeddings[key][name][\"relative_grammaticality\"] = ev\n",
    "        #Probability of whole sequence\n",
    "        embeddings[key][name]['sequence_grammaticality'] = get_sequence_grammaticality(sequence,embeddings[key][name]['Logits'])\n",
    "#             print('Sequence Grammaticality: ', embeddings[key][name]['sequence_grammaticality'])\n",
    "        #Probability ratio between the mutant sequence and the reference sequence\n",
    "        embeddings[key][name]['relative_sequence_grammaticality'] = embeddings[key][name]['sequence_grammaticality']-embeddings[key][\"Reference\"]['sequence_grammaticality']\n",
    "#             print('Relative Sequence Grammaticality: ', embeddings[key][name]['relative_sequence_grammaticality'])\n",
    "        embeddings[key][name][\"probability\"] = np.exp(gm)\n",
    "#             print(embeddings[key][name]['grammaticality'])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7895fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322a660",
   "metadata": {
    "id": "cd0299b9-a298-4872-a8f8-8f7129dbc748",
    "tags": []
   },
   "source": [
    "# Load Model into GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8e8937",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "60afd8d4-1ec3-4db6-aa06-fb15b86c0bb7",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "83d0799e-dab7-42cb-fd81-3ec124be724c"
   },
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.load_model_and_alphabet(\"esm2_t33_650M_UR50D\") # 'esm2_t36_3B_UR50D' is too large for my system atm\n",
    "model.eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "device = torch.device(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    model =  model.to(device)\n",
    "    print(\"Transferred model to GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d1bfe",
   "metadata": {
    "id": "9b3207e2-bfa4-4928-aed5-4097adb50597",
    "tags": []
   },
   "source": [
    "# Download Reference Sequence and Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00934ade",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": false,
    "id": "350c0992-ea0c-459f-95a0-18eeebd5729b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_layers = 33 # smaller model only has 22 layers, not 36 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86c599a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reference_protein = 'TTSAGESADPVTATVENYGGETQVQRRQHTDIAFILDRFVKVKPKEQVNVLDLMQIPAHTLVGALLRTATYYFSDLELAVKHEGDLTWVPNGAPETALDNTTNPTAYHKEPLTRLALPYTAPHRVLATVYNGSSKYGDTSTNNVRGDLQVLAQKAERTLPTSFNFGAIKATRVTELLYRMKRAETYCPRPLLAIQPSDARHKQRIVAPAKQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6452fd27",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "50e64d10-d0da-4620-a162-02525b1baeea",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "43649cf2-488d-4ef6-ff0b-43a11143ed95"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81c9d8d131641c8a0ecea562180bcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutations:  ['T1A']\n",
      "Mutations:  ['T1R']\n",
      "Mutations:  ['T1N']\n",
      "Mutations:  ['T1D']\n",
      "Mutations:  ['T1C']\n",
      "Mutations:  ['T1Q']\n",
      "Mutations:  ['T1E']\n",
      "Mutations:  ['T1G']\n",
      "Mutations:  ['T1H']\n",
      "Mutations:  ['T1I']\n",
      "Mutations:  ['T1L']\n",
      "Mutations:  ['T1K']\n",
      "Mutations:  ['T1M']\n",
      "Mutations:  ['T1F']\n",
      "Mutations:  ['T1P']\n",
      "Mutations:  ['T1S']\n",
      "Mutations:  ['T1T']\n",
      "Mutations:  ['T1W']\n",
      "Mutations:  ['T1Y']\n",
      "Mutations:  ['T1V']\n",
      "Mutations:  ['T2A']\n",
      "Mutations:  ['T2R']\n",
      "Mutations:  ['T2N']\n",
      "Mutations:  ['T2D']\n",
      "Mutations:  ['T2C']\n",
      "Mutations:  ['T2Q']\n",
      "Mutations:  ['T2E']\n",
      "Mutations:  ['T2G']\n",
      "Mutations:  ['T2H']\n",
      "Mutations:  ['T2I']\n",
      "Mutations:  ['T2L']\n",
      "Mutations:  ['T2K']\n",
      "Mutations:  ['T2M']\n",
      "Mutations:  ['T2F']\n",
      "Mutations:  ['T2P']\n",
      "Mutations:  ['T2S']\n",
      "Mutations:  ['T2T']\n",
      "Mutations:  ['T2W']\n",
      "Mutations:  ['T2Y']\n",
      "Mutations:  ['T2V']\n",
      "Mutations:  ['S3A']\n",
      "Mutations:  ['S3R']\n",
      "Mutations:  ['S3N']\n",
      "Mutations:  ['S3D']\n",
      "Mutations:  ['S3C']\n",
      "Mutations:  ['S3Q']\n",
      "Mutations:  ['S3E']\n",
      "Mutations:  ['S3G']\n",
      "Mutations:  ['S3H']\n",
      "Mutations:  ['S3I']\n",
      "Mutations:  ['S3L']\n",
      "Mutations:  ['S3K']\n",
      "Mutations:  ['S3M']\n",
      "Mutations:  ['S3F']\n",
      "Mutations:  ['S3P']\n",
      "Mutations:  ['S3S']\n",
      "Mutations:  ['S3T']\n",
      "Mutations:  ['S3W']\n",
      "Mutations:  ['S3Y']\n",
      "Mutations:  ['S3V']\n",
      "Mutations:  ['A4A']\n",
      "Mutations:  ['A4R']\n",
      "Mutations:  ['A4N']\n",
      "Mutations:  ['A4D']\n",
      "Mutations:  ['A4C']\n",
      "Mutations:  ['A4Q']\n",
      "Mutations:  ['A4E']\n",
      "Mutations:  ['A4G']\n",
      "Mutations:  ['A4H']\n",
      "Mutations:  ['A4I']\n",
      "Mutations:  ['A4L']\n",
      "Mutations:  ['A4K']\n",
      "Mutations:  ['A4M']\n",
      "Mutations:  ['A4F']\n",
      "Mutations:  ['A4P']\n",
      "Mutations:  ['A4S']\n",
      "Mutations:  ['A4T']\n",
      "Mutations:  ['A4W']\n",
      "Mutations:  ['A4Y']\n",
      "Mutations:  ['A4V']\n",
      "Mutations:  ['G5A']\n",
      "Mutations:  ['G5R']\n",
      "Mutations:  ['G5N']\n",
      "Mutations:  ['G5D']\n",
      "Mutations:  ['G5C']\n",
      "Mutations:  ['G5Q']\n",
      "Mutations:  ['G5E']\n",
      "Mutations:  ['G5G']\n",
      "Mutations:  ['G5H']\n",
      "Mutations:  ['G5I']\n",
      "Mutations:  ['G5L']\n",
      "Mutations:  ['G5K']\n",
      "Mutations:  ['G5M']\n",
      "Mutations:  ['G5F']\n",
      "Mutations:  ['G5P']\n",
      "Mutations:  ['G5S']\n",
      "Mutations:  ['G5T']\n",
      "Mutations:  ['G5W']\n",
      "Mutations:  ['G5Y']\n",
      "Mutations:  ['G5V']\n",
      "Mutations:  ['E6A']\n",
      "Mutations:  ['E6R']\n",
      "Mutations:  ['E6N']\n",
      "Mutations:  ['E6D']\n",
      "Mutations:  ['E6C']\n",
      "Mutations:  ['E6Q']\n",
      "Mutations:  ['E6E']\n",
      "Mutations:  ['E6G']\n",
      "Mutations:  ['E6H']\n",
      "Mutations:  ['E6I']\n",
      "Mutations:  ['E6L']\n",
      "Mutations:  ['E6K']\n",
      "Mutations:  ['E6M']\n",
      "Mutations:  ['E6F']\n",
      "Mutations:  ['E6P']\n",
      "Mutations:  ['E6S']\n",
      "Mutations:  ['E6T']\n",
      "Mutations:  ['E6W']\n",
      "Mutations:  ['E6Y']\n",
      "Mutations:  ['E6V']\n",
      "Mutations:  ['S7A']\n",
      "Mutations:  ['S7R']\n",
      "Mutations:  ['S7N']\n",
      "Mutations:  ['S7D']\n",
      "Mutations:  ['S7C']\n",
      "Mutations:  ['S7Q']\n",
      "Mutations:  ['S7E']\n",
      "Mutations:  ['S7G']\n",
      "Mutations:  ['S7H']\n",
      "Mutations:  ['S7I']\n",
      "Mutations:  ['S7L']\n",
      "Mutations:  ['S7K']\n",
      "Mutations:  ['S7M']\n",
      "Mutations:  ['S7F']\n",
      "Mutations:  ['S7P']\n",
      "Mutations:  ['S7S']\n",
      "Mutations:  ['S7T']\n",
      "Mutations:  ['S7W']\n",
      "Mutations:  ['S7Y']\n",
      "Mutations:  ['S7V']\n"
     ]
    }
   ],
   "source": [
    "dms_results = single_protein_DMS('FMDV_Reference_VP1',reference_protein,model,model_layers,batch_converter,alphabet,device,0,6) # cap to the first 6 positions instead of None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e323028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Reference', 'T1A', 'T1R', 'T1N', 'T1D', 'T1C', 'T1Q', 'T1E', 'T1G', 'T1H', 'T1I', 'T1L', 'T1K', 'T1M', 'T1F', 'T1P', 'T1S', 'T1T', 'T1W', 'T1Y', 'T1V', 'T2A', 'T2R', 'T2N', 'T2D', 'T2C', 'T2Q', 'T2E', 'T2G', 'T2H', 'T2I', 'T2L', 'T2K', 'T2M', 'T2F', 'T2P', 'T2S', 'T2T', 'T2W', 'T2Y', 'T2V', 'S3A', 'S3R', 'S3N', 'S3D', 'S3C', 'S3Q', 'S3E', 'S3G', 'S3H', 'S3I', 'S3L', 'S3K', 'S3M', 'S3F', 'S3P', 'S3S', 'S3T', 'S3W', 'S3Y', 'S3V', 'A4A', 'A4R', 'A4N', 'A4D', 'A4C', 'A4Q', 'A4E', 'A4G', 'A4H', 'A4I', 'A4L', 'A4K', 'A4M', 'A4F', 'A4P', 'A4S', 'A4T', 'A4W', 'A4Y', 'A4V', 'G5A', 'G5R', 'G5N', 'G5D', 'G5C', 'G5Q', 'G5E', 'G5G', 'G5H', 'G5I', 'G5L', 'G5K', 'G5M', 'G5F', 'G5P', 'G5S', 'G5T', 'G5W', 'G5Y', 'G5V', 'E6A', 'E6R', 'E6N', 'E6D', 'E6C', 'E6Q', 'E6E', 'E6G', 'E6H', 'E6I', 'E6L', 'E6K', 'E6M', 'E6F', 'E6P', 'E6S', 'E6T', 'E6W', 'E6Y', 'E6V', 'S7A', 'S7R', 'S7N', 'S7D', 'S7C', 'S7Q', 'S7E', 'S7G', 'S7H', 'S7I', 'S7L', 'S7K', 'S7M', 'S7F', 'S7P', 'S7S', 'S7T', 'S7W', 'S7Y', 'S7V'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dms_results['FMDV_Reference_VP1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300ca2da",
   "metadata": {
    "collapsed": false,
    "id": "fa8cd531-9062-4d70-8a45-8cf94d78e627",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "compressed_pickle('FMDV_Reference_VP1',dms_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d53c663a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dms_results=decompress_pickle('FMDV_Reference_VP1.pbz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c890b94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mutations_list = list(dms_results['FMDV_Reference_VP1'].keys())\n",
    "columns = ['label', 'semantic_score', 'grammaticality', 'relative_grammaticality', 'sequence_grammaticality', 'relative_sequence_grammaticality', 'probability']\n",
    "table = []\n",
    "for key in mutations_list:\n",
    "    if key != 'Reference':\n",
    "        row = pd.DataFrame([dms_results['FMDV_Reference_VP1'][key].get(c) for c in columns]).T\n",
    "        row.columns = columns\n",
    "        table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4969fa21",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dms_table = pd.concat(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233cc2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annotate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5028d06b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dms_table['ref'] = dms_table.label.str[0]\n",
    "dms_table['alt'] = dms_table.label.str[-1]\n",
    "dms_table['position'] = dms_table.label.str[1:-1].astype(int)\n",
    "\n",
    "#Keep Reference scores\n",
    "reference_s_table = dms_table[dms_table.ref == dms_table.alt]\n",
    "#Filter non mutations\n",
    "dms_table = dms_table[dms_table.ref != dms_table.alt]\n",
    "\n",
    "\n",
    "dms_table = dms_table.sort_values('semantic_score')\n",
    "dms_table['semantic_rank'] = dms_table.reset_index().index.astype(int) + 1\n",
    "dms_table = dms_table.sort_values('grammaticality')\n",
    "dms_table['grammatical_rank'] =dms_table .reset_index().index.astype(int) + 1\n",
    "dms_table['acquisition_priority'] = dms_table['semantic_rank'] + dms_table['grammatical_rank']\n",
    "\n",
    "dms_table = dms_table.sort_values('sequence_grammaticality')\n",
    "dms_table['sequence_grammatical_rank'] =dms_table.reset_index().index.astype(int) + 1\n",
    "dms_table['sequence_acquisition_priority'] = dms_table['semantic_rank'] + dms_table['sequence_grammatical_rank']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3147d44d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fmdv_domain_annotation(position):\n",
    "    if position>=137 and position <=143: # 6 positions we want to focus on mutations of?\n",
    "        return \"GH Loop\"\n",
    "    elif position>=145 and position <=147:\n",
    "        return \"RDG Motif\"\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e554fe7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dms_table[\"Domain\"] = \"\"\n",
    "dms_table[\"Domain\"] = [ fmdv_domain_annotation(pos) for pos in dms_table[\"position\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10b0a857",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dms_table.sort_values('position').to_csv('FMDV_Reference_VP1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72f6b8-02bd-414c-be5a-cc526a64ccdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EIDK9H-dsj4-",
    "dvifJu39sg89",
    "_6-DG2B3seTT",
    "cb_ttyiKsKOb"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
